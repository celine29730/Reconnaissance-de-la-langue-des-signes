{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance de la langue des signes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte du projet\n",
    "\n",
    "Beaucoup de progrès et de recherches en IA ont été faites pour aider les personnes sourdes et muettes. L'apprentissage profond et la vision par ordinateur peuvent également être utilisés pour avoir un impact sur cette cause.\n",
    "\n",
    "Cela peut être très utile pour les personnes sourdes et muettes dans la communication avec les autres car la connaissance de la langue des signes n'est pas quelque chose qui est commun à tous, de plus, cela peut être étendu à la création des éditeurs automatiques, où la personne peut facilement écrire par ses simples gestes .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utiliserons l'aphabet du langage des signes suivant:\n",
    "\n",
    "![alphabet](alphabet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet de reconnaissance de la langue des signes, nous créons un détecteur de signes, qui détecte les lettres de l'alphabet qui peuvent très facilement être étendus pour couvrir une vaste multitude d'autres signes et gestes de la main.\n",
    "\n",
    "On va utiliser pour ce projet les modules OpenCV et Keras de python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le projet est divisé en 3 parties:\n",
    "\n",
    "    1. Création du jeu de données\n",
    "    2. Entraîner un CNN sur l'ensemble de données capturé\n",
    "    3. Prédire les données\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du Dataset\n",
    "\n",
    "Dans cette première partie, nous constituons un dataset avec les différents signes de l'alphabet. Nous avons choisi de collecter des images traitées en amont via un \"filtre\" (Thresholded) qui permettra de différencier au mieux chaque signe effectué avec la main.\n",
    "\n",
    "Durant cette aquisition de donnée, nous chargeons préalablement le background, qui ne sera pas pris en compte lorsque nous positionnerons la main. Une fois fait, 300 images seront enregistrées dans un dossier conçu à cet effet. L'opération sera reproduite pour chaque catégorie (chaque signe de l'alphabet).\n",
    "\n",
    "Le dataset constitué (près de 8100 images) sera compressé pour l'importer sur *Colab* afin d'entrainer notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est assez possible d'obtenir l'ensemble de données dont nous avons besoin sur Internet, mais dans ce projet, \n",
    "nous créerons l'ensemble de données nous-mêmes.\n",
    "\n",
    "Nous aurons un flux en direct de la caméra vidéo et chaque image qui détecte une main dans le ROI \n",
    "(région d'intérêt) créée sera enregistrée dans un répertoire (ici le répertoire dataset) qui contientra les \n",
    "répertoires de toutes les lettres de l'alphabet ainsi que l'espace (_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, pour créer l'ensemble de données, nous obtenons le flux de caméra en direct à l'aide d'OpenCV et \n",
    "créons un retour sur investissement qui n'est rien d'autre que la partie de l'image où nous voulons détecter \n",
    "la main pour les gestes.\n",
    "\n",
    "La boîte rouge est le retour sur investissement et cette fenêtre sert à obtenir le flux de la caméra en direct \n",
    "de la webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour différencier l'arrière-plan, nous calculons la moyenne pondérée accumulée pour l'arrière-plan, puis soustrayons celle-ci des cadres qui contiennent un objet devant l'arrière-plan qui peut être distingué comme premier plan.\n",
    "\n",
    "Cela se fait en calculant le cumulated_weight pour certaines images (ici pour 60 images), nous calculons accumulated_avg pour l'arrière-plan.\n",
    "\n",
    "Une fois que nous avons la moyenne accumulée pour l'arrière-plan, nous la soustrayons de chaque image que nous lisons après 60 images pour trouver tout objet qui couvre l'arrière-plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = None\n",
    "accumulated_weight = 0.9\n",
    "\n",
    "# Création des dimensions du ROI\n",
    "\n",
    "ROI_top = 50\n",
    "ROI_bottom = 300\n",
    "ROI_right = 50\n",
    "ROI_left = 300\n",
    "\n",
    "# Project: gesture-recognition, License: MIT License, OpenSource\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    \n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Nous mettons en place un texte en utilisant cv2.putText pour afficher pour attendre et ne mettre aucun objet ou \n",
    "main dans le ROI lors de la détection de l'arrière-plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la valeur seuil\n",
    "\n",
    "Nous calculons maintenant la valeur de seuil pour chaque image et déterminons les contours à l'aide de \n",
    "cv2.findContours et nous renvoyons les contours max (les contours les plus extérieurs pour l'objet) à l'aide du \n",
    "segment de fonction. En utilisant les contours, nous sommes en mesure de déterminer s'il y a un objet de premier \n",
    "plan détecté dans le ROI, en d'autres termes, s'il y a une main dans le ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'application du filtre (Thresholded)\n",
    "def segment_hand(frame, threshold=20):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    thresholded = cv2.adaptiveThreshold(diff,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n",
    "    \n",
    "    return (thresholded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque des contours sont détectés (ou qu'une main est présente dans la ROI), nous commençons à enregistrer \n",
    "l'image de la ROI dans le répertoire \"dataset\" pour la lettre ou le chiffre pour lequel nous la détectons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas oublier de créer au préalable un dossier pour chaque lettre de l'alphabet et _SPACE dans le répertoire \"dataset\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouverture de la caméra\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# Définition des variables (nombre de frame, dossier d'images)\n",
    "num_frames = 0\n",
    "element = '_SPACE'\n",
    "\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    \n",
    "    # Eviter l'inversement de l'image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = frame.copy()\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "    \n",
    "    # Traitement\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)\n",
    "    \n",
    "    if num_frames < 100:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        \n",
    "        # Détection du Background\n",
    "        if num_frames <= 299:\n",
    "            cv2.putText(frame_copy, \"Chargement..\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            \n",
    "    # Configuration (Nous pouvons mettre notre main et la positionner correctement avant enregistrement)\n",
    "    elif num_frames <= 300: \n",
    "        hand = segment_hand(gray_frame)\n",
    "        cv2.putText(frame_copy, \"Configuration..\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0),2)\n",
    "        \n",
    "        # Voir si la main est bien détectée\n",
    "        if hand is not None:\n",
    "            thresholded = hand\n",
    "            \n",
    "            # Dessiner les contours de la main\n",
    "            cv2.putText(frame_copy, str(num_frames)+\" Pour \"+ str(element),(70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
    "            \n",
    "            # Affiche l'image prise\n",
    "            cv2.imshow(\"Image traitée\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        # Filtre sur la main\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Repérer si la main est là\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded = hand\n",
    "            \n",
    "            # Détection de la main (avec le filtre)\n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'Enregistrement pour ' + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            \n",
    "            # Enregistrement des images de la main traitée dans la frame Filtre\n",
    "            cv2.imshow(\"Image traitée\", thresholded)\n",
    "            if num_imgs_taken <= 299:\n",
    "                cv2.imwrite(r\"C:\\\\Users\\\\utilisateur\\\\Google Drive\\\\microsoft_ia\\\\Reconnaissance de la langue des signes\\\\projetfinal\\\\dataset\\\\\"+str(element)+\"\\\\\"+str(num_imgs_taken+1) +'.jpg', thresholded)\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "            \n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'Pas de main détectée !', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    # Dessine une ROI de la frame Filtre\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    # Incrementation\n",
    "    num_frames += 1\n",
    "    # Montre la frame Filtre\n",
    "    cv2.imshow(\"Filtre\", frame_copy)\n",
    "    # Ferme l'application\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
